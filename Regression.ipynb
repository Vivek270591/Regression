{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "\n",
        "Simple Linear Regression is a statistical method that models the relationship between two continuous variables: one independent variable (X) and one dependent variable (Y). It assumes a linear relationship and fits a straight line (Y = mX + c) to the data that minimizes the difference between the observed and predicted values.\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "- Linearity between independent and dependent variable\n",
        "- Homoscedasticity (constant variance of residuals)\n",
        "- Independence of observations\n",
        "- Normal distribution of residuals\n",
        "- No significant outliers\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "\n",
        "m is the slope of the regression line. It represents the change in the dependent variable (Y) for a one-unit change in the independent variable (X).\n",
        "\n",
        "4. What does the intercept c represent in the equation Y = mX + c?\n",
        "\n",
        "c is the intercept, or the value of Y when X = 0. It indicates the starting value of Y in the absence of X.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "m = Σ((Xi - X̄)(Yi - Ȳ)) / Σ((Xi - X̄)^2) — the ratio of the covariance of X and Y to the variance of X.\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "It minimizes the sum of squared residuals (differences between observed and predicted Y values), ensuring the best-fitting regression line.\n",
        "\n",
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "\n",
        "R² indicates the proportion of variance in the dependent variable that is explained by the independent variable. R² = 1 implies perfect prediction; R² = 0 means no predictive power.\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "\n",
        "It models the relationship between one dependent variable and two or more independent variables using the equation: Y = b0 + b1X1 + b2X2 + ... + bnXn.\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "Simple Linear Regression involves one independent variable, whereas Multiple Linear Regression involves two or more independent variables.\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "- Linearity between each predictor and the response\n",
        "- Multivariate normality\n",
        "- No or little multicollinearity\n",
        "- Homoscedasticity\n",
        "- Independence of errors\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Heteroscedasticity refers to non-constant variance of residuals. It violates regression assumptions and can lead to inefficient estimates and biased standard errors.\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "- Remove or combine correlated variables\n",
        "- Use Principal Component Analysis (PCA)\n",
        "- Apply Ridge or Lasso regression\n",
        "- Center the variables (mean subtraction)\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "- One-hot encoding\n",
        "- Label encoding\n",
        "- Dummy variable creation\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Interaction terms (e.g., X₁×X₂) capture the effect of variables interacting with each other, helping to model complex relationships that are not purely additive.\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "In Simple Linear Regression, the intercept is Y when X = 0. In Multiple Linear Regression, it’s the expected Y value when all predictors are 0, which may not be meaningful if 0 is outside the data range.\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "The slope quantifies the effect of a one-unit change in the predictor on the response variable. It directly influences the predictions made by the regression model.\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "It establishes the baseline value of the dependent variable and provides context for how predictions behave when all independent variables are zero.\n",
        "\n",
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "- Doesn’t indicate whether a regression model is appropriate\n",
        "- Can be artificially high with more variables (overfitting)\n",
        "- Doesn’t reveal bias or variance issues\n",
        "\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "A large standard error implies that the coefficient estimate is not precise and may not be statistically significant.\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "If residual plots show a funnel shape (residuals increasing/decreasing with fitted values), it indicates heteroscedasticity. Addressing it is essential to ensure valid inference and accurate confidence intervals.\n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        "It suggests that some predictors do not add meaningful value, and the model may be overfitting the data.\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "Scaling ensures comparability of coefficients, improves convergence of gradient-based methods, and is crucial when using regularization.\n",
        "\n",
        "23. What is polynomial regression?\n",
        "\n",
        "Polynomial Regression is an extension of linear regression where the relationship between the independent variable and the dependent variable is modeled as an nth-degree polynomial.\n",
        "\n",
        "24. How does polynomial regression differ from linear regression?\n",
        "\n",
        "Linear regression models straight-line relationships, while polynomial regression can model curved (nonlinear) relationships.\n",
        "\n",
        "25. When is polynomial regression used?\n",
        "\n",
        "When data shows a nonlinear relationship that cannot be captured by a straight line.\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        "\n",
        "Y = b₀ + b₁X + b₂X² + ... + bₙXⁿ\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Yes, by including polynomial terms for each variable and their interactions, it becomes a multivariate polynomial regression.\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        "\n",
        "- Overfitting at high degrees\n",
        "- Sensitive to outliers\n",
        "- Poor extrapolation behavior\n",
        "- Complexity in interpretation\n",
        "\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "- Cross-validation\n",
        "- Adjusted R²\n",
        "- AIC/BIC\n",
        "- Residual analysis\n",
        "\n",
        "30. Why is visualization important in polynomial regression?\n",
        "\n",
        "It helps in understanding the fit of the model, detecting overfitting/underfitting, and conveying model behavior intuitively.\n"
      ],
      "metadata": {
        "id": "ZkEnAAcsv2vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#31. How is polynomial regression implemented in Python?\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Example dataset\n",
        "import numpy as np\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([1, 4, 9, 16, 25])  # y = x^2\n",
        "\n",
        "# Create a model with polynomial features\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X)\n",
        "print(predictions)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erQQsnbbwfTr",
        "outputId": "9ac66e85-3893-4e6b-ff57-e720c053b6d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.  4.  9. 16. 25.]\n"
          ]
        }
      ]
    }
  ]
}